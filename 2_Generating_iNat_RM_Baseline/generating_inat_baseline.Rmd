---
title: "Generating baseline from iNat data to compare users"
author: "Biohack team 12"
date: "09/11/2021"
output: html_document
---


```{r}
#devtools::install_github('biologicalrecordscentre/recorderMetrics')

library(rinat)

library(adehabitatHR)
library(raster)
library(sp)
library(rgdal)
library(recorderMetrics)

library(dplyr)
library(purrr)

library(httr)
library(jsonlite)

library(sf)

#library(ggplot2)

library(stringr)

library(geosphere)

```

## Get data

### Getting a sample of users


Provide information about how we download the data

Turn users into usernames

```{r}
#load users
recorded_by <- readLines("Getting_Random_Observers/data/spain_100obs_observers_2021-11-09_12-03.txt")

usernames <- c()

for (i in 1:length(recorded_by)){
  res = GET("https://api.inaturalist.org/v1/users/autocomplete",
    query = list(q = recorded_by[i]))
  
  data = fromJSON(rawToChar(res$content))
  
  if (nrow(data$results)>1) {
    print("multiple matches :S")
  } else if (nrow(data$results)==1) {
    print("one match good! :D")
    username <- data$results$login
    usernames <- append(usernames,username)
  } else {
    "no matches :("
  }
  Sys.sleep(1)
}

saveRDS(usernames,"2_Generating_iNat_RM_Baseline/data/usernames.rds")

```



### Get observations for users

Do get calls for each user


Edit the get_inat_obs_user to be able to query by extra fields

```{r}

get_inat_obs_user_tweaked <- function (username, maxresults = 100,queryextra) 
{
    if (!curl::has_internet()) {
        message("No Internet connection.")
        return(invisible(NULL))
    }
    base_url <- "http://www.inaturalist.org/"
    if (httr::http_error(base_url)) {
        message("iNaturalist API is unavailable.")
        return(invisible(NULL))
    }
    q_path <- paste0(username, ".csv")
    ping_path <- paste0(username, ".json")
    ping_query <- paste0("&per_page=1&page=1",queryextra)
    ping <- GET(base_url, path = paste0("observations/", 
        ping_path), query = ping_query)
    total_res <- as.numeric(ping$headers$`x-total-entries`)
    if (total_res == 0) {
        stop("Your search returned zero results. Perhaps your user does not exist.")
    }
    page_query <- paste0("&per_page=200&page=1", queryextra)
    dat <- GET(base_url, path = paste0("observations/", 
        q_path), query = page_query)
    data_out <- read.csv(textConnection(content(dat, as = "text")))
    if (maxresults > 200) {
        for (i in 2:ceiling(total_res/200)) {
            page_query <- paste0("&per_page=200&page=", 
                i, queryextra)
            dat <- GET(base_url, path = paste0("observations/", 
                q_path), query = page_query)
            data_out <- rbind(data_out, read.csv(textConnection(content(dat, 
                as = "text"))))
            Sys.sleep(0.1)
        }
    }
    if (maxresults < dim(data_out)[1]) {
        data_out <- data_out[1:maxresults, ]
    }
    return(data_out)
}

```

```{r}
#load usernames from whatever source
usernames <- readRDS("2_Generating_iNat_RM_Baseline/data/usernames.rds")

for (i in 1:length(usernames)){
  # using function adapted from rinat package
  print(i)
  tryCatch({
    user_observations <- get_inat_obs_user_tweaked(usernames[i],maxresults = 5000,queryextra = "&taxon_id=47126&place_id=6774")
    saveRDS(user_observations,paste0("2_Generating_iNat_RM_Baseline/data/user_obvs/observations_",usernames[i],".rds"))
    },
    error = function(cond){
      print(cond)
    })
  
  Sys.sleep(1)
}

# NOT IN USE:
#new approach using: https://api.inaturalist.org/v1/docs/#!/Observations/get_observations
# res = GET("https://api.inaturalist.org/v1/observations",
#   query = list(user_id = recorded_by[2],
#                per_page = 5000,
#                search_on = "place",
#                q = "Spain"))
# 
# data = fromJSON(rawToChar(res$content))
# data$results %>% nrow()
# saveRDS(user_observations,paste0("2_Generating_iNat_RM_Baseline/data/user_obvs/observations_",usernames[i],".rds"))
```

Combine into one dataframe

```{r}
all_observations_raw <- paste0("2_Generating_iNat_RM_Baseline/data/user_obvs/",list.files(path = "2_Generating_iNat_RM_Baseline/data/user_obvs", pattern = ".rds")) %>%
  map(readRDS) %>% 
  bind_rows()

```

Format data ready for recorderMetrics

```{r}
#see what columns we have to play with
all_observations_raw %>% str()

#get columns and rename
all_observations <- all_observations_raw %>% 
  select(id = id,
         recorder = user_login,
         species = scientific_name,
         date = observed_on,
         long = longitude,
         lat = latitude,
         quality_grade = quality_grade) %>%
  mutate(species_level = F,date = as.Date(date)) %>%
  filter(!is.na(long),!is.na(lat),!is.na(date)) #remove observations without locaton or date

all_observations$species_level[str_detect(all_observations$species, " ")] <- T

#quick look at the data
all_observations %>% str()

#generating sites
#option 1: 1km grid squares using spanish national grid

# generate: km_sq by projecting to spanish national grid
xy <- all_observations %>% select(long,lat)

#make into spatial data frame, convert to sf, transform to spanish grid, get coordinates and round to nearest 1000
spdf <- SpatialPointsDataFrame(coords = xy, data = all_observations,
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) %>% 
  st_as_sf() %>% 
  st_transform(spdf,crs = 2062) %>% 
  st_coordinates() %>% 
  round(-3) # rounds to nearest 1000m, not rounding down

#add the column to the data frame
all_observations$km_sq <- paste0(spdf[,1],",",spdf[,2])

# option 2: clustering
spdf <- SpatialPointsDataFrame(coords = xy, data = all_observations,
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
mdist <- distm(spdf) # doesn't work because it wants to make a massive vector

hc <- hclust(as.dist(mdist), method="complete")

# define the distance threshold, in this case 40 m
d=100

# define clusters based on a tree "height" cutoff "d" and add them to the SpDataFrame
cutree(hc, h=d)


write.csv(all_observations,"2_Generating_iNat_RM_Baseline/data/observations/draft_data.csv")
saveRDS(all_observations,"2_Generating_iNat_RM_Baseline/data/observations/draft_data.rds")
```

Checking out the data

```{r}
record_count <- all_observations %>% group_by(recorder) %>% summarise(n_records = n())

```


## Metric-ify

### Plug into recorder metrics

```{r}

all_observations <- readRDS("2_Generating_iNat_RM_Baseline/data/observations/draft_data.rds")

#not sure if this works
metrics_axes <- predictAxes(data = all_observations,
                            recorders  = unique(all_observations$recorder))
# 
# str(metrics_axes)




ar <- activityRatio(recorder_name = unique(all_observations$recorder)[1],
                   data = all_observations,
                   recorder_col = 'recorder',
                   date_col = 'date')

LL <- listLength(data = all_observations,
                 recorder_name = unique(all_observations$recorder)[1],
                 threshold = 10,
                 plot = T,
                 sp_col = 'species',
                 date_col = 'date',
                 recorder_col = 'recorder',
                 location_col = 'km_sq')


```


### do stuff









